{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"OqQP3OSzSpzC","executionInfo":{"status":"ok","timestamp":1761063254496,"user_tz":-330,"elapsed":13509,"user":{"displayName":"GANESH S 231501046","userId":"12955083252446110846"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","image_size = 64\n","batch_size = 128\n","latent_dim = 100\n","num_epochs = 5\n","learning_rate = 1e-3\n","\n","# Define transforms for both CelebA and MNIST\n","transform_celeba = transforms.Compose([\n","  transforms.Resize(image_size),\n","  transforms.CenterCrop(image_size), # CelebA images are often cropped\n","  transforms.ToTensor(),\n","  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize for RGB channels\n","])\n","\n","transform_mnist = transforms.Compose([\n","  transforms.Resize(image_size),\n","  transforms.ToTensor(),\n","  transforms.Normalize((0.5,), (0.5,)) # Normalize for single channel - Corrected\n","])\n","\n","# Define a placeholder CelebADatasetManual class if it's not defined elsewhere\n","class CelebADatasetManual(torch.utils.data.Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        # Assuming the images are directly in the root_dir and are in a format\n","        # that torchvision.io.read_image can handle (e.g., .jpg, .png)\n","        # You might need to adjust this based on your actual CelebA file structure.\n","        self.image_files = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_path = self.image_files[idx]\n","        # Use torchvision.io to read the image\n","        # This requires installing torchvision with io capabilities (e.g., pip install torchvision)\n","        try:\n","            image = torchvision.io.read_image(img_path)\n","            # Convert grayscale to RGB if needed (e.g., for consistency with model input)\n","            if image.shape[0] == 1:\n","                image = image.repeat(3, 1, 1)\n","            # Convert from uint8 to float and scale to [0, 1]\n","            image = image.float() / 255.0\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","            # Return a dummy image or handle the error as appropriate\n","            # Returning a tuple of dummy image and label\n","            return torch.zeros((3, image_size, image_size)), 0\n","\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # CelebA dataset typically doesn't have explicit class labels like MNIST,\n","        # but you might have attribute labels. For this VAE example, we can\n","        # just return a dummy label or modify the dataloader loop to only expect images.\n","        # Returning a tuple of image and dummy label\n","        label = 0\n","\n","        return image, label\n","\n","\n","# Check if CelebA dataset is available and use it, otherwise use MNIST\n","celeba_root_dir = '/content/img_align_celeba' # Update this path if needed\n","if os.path.exists(celeba_root_dir):\n","    print(\"Using CelebA dataset\")\n","    dataset = CelebADatasetManual(root_dir=celeba_root_dir, transform=transform_celeba)\n","else:\n","    print(\"CelebA dataset not found, using MNIST dataset\")\n","    dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform_mnist)\n","    # dataset = Subset(dataset, range(500)) # Uncomment to use a smaller subset of MNIST for faster testing\n","\n","\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-sTui0JSsa1","executionInfo":{"status":"ok","timestamp":1761063260687,"user_tz":-330,"elapsed":3052,"user":{"displayName":"GANESH S 231501046","userId":"12955083252446110846"}},"outputId":"d9a8cc6c-4b03-415c-a3fe-a315f4812558"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CelebA dataset not found, using MNIST dataset\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 20.5MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 481kB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 4.74MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 9.34MB/s]\n"]}]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","  def __init__(self, latent_dim):\n","    super(Encoder, self).__init__()\n","    # The first convolutional layer's input channels will be determined by the dataset loaded\n","    # We'll keep it at 3 for CelebA and adjust if MNIST is used.\n","    # However, since we are now switching the transform based on the dataset,\n","    # the input channels to the model should match the output channels of the transform.\n","    # The transforms are set up to output 3 channels for CelebA and 1 for MNIST.\n","    # We need to make the Encoder flexible or ensure the correct transform is applied.\n","    # Let's keep the Encoder for 3 channels as the primary goal is CelebA,\n","    # and ensure the MNIST transform outputs 3 channels by repeating the single channel.\n","\n","    # *** Correction: It's better to modify the Encoder's first layer based on the dataset. ***\n","    # We will handle this by creating the model *after* the dataloader is set up,\n","    # and pass the number of input channels to the Encoder.\n","\n","    self.conv = nn.Sequential(\n","      nn.Conv2d(3, 64, 4, 2, 1), nn.ReLU(), # Keep at 3 channels for CelebA\n","      nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),\n","      nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),\n","      nn.Conv2d(256, 512, 4, 2, 1), nn.BatchNorm2d(512), nn.ReLU()\n","    )\n","    self.fc_mu = nn.Linear(512*4*4, latent_dim)\n","    self.fc_logvar = nn.Linear(512*4*4, latent_dim)\n","\n","  def forward(self, x):\n","    x = self.conv(x)\n","    x = x.view(x.size(0), -1)\n","    return self.fc_mu(x), self.fc_logvar(x)"],"metadata":{"id":"BY1bhNwESypS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","  def __init__(self, latent_dim):\n","    super(Decoder, self).__init__()\n","    self.fc = nn.Linear(latent_dim, 512*4*4)\n","    self.deconv = nn.Sequential(\n","      nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),\n","      nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),\n","      nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),\n","      nn.ConvTranspose2d(64, 3, 4, 2, 1), nn.Tanh() # Keep at 3 channels for CelebA\n","    )\n","\n","  def forward(self, z):\n","    x = self.fc(z)\n","    x = x.view(x.size(0), 512, 4, 4)\n","    return self.deconv(x)"],"metadata":{"id":"RBHK7b2qTrI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VAE(nn.Module):\n","  def __init__(self, latent_dim):\n","    super(VAE, self).__init__()\n","    self.encoder = Encoder(latent_dim)\n","    self.decoder = Decoder(latent_dim)\n","\n","  def reparameterize(self, mu, logvar):\n","    std = torch.exp(0.5 * logvar)\n","    eps = torch.randn_like(std)\n","    return mu + eps * std\n","\n","  def forward(self, x):\n","    mu, logvar = self.encoder(x)\n","    z = self.reparameterize(mu, logvar)\n","    return self.decoder(z), mu, logvar"],"metadata":{"id":"u2MSXAv8T9XD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def vae_loss(recon_x, x, mu, logvar):\n","    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n","    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return recon_loss + kl_div"],"metadata":{"id":"I4cuAnbHUGhE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Determine input channels based on the dataset\n","input_channels = 3 if isinstance(dataset, CelebADatasetManual) else 1\n","\n","class Encoder(nn.Module):\n","  def __init__(self, latent_dim, in_channels):\n","    super(Encoder, self).__init__()\n","    self.conv = nn.Sequential(\n","      nn.Conv2d(in_channels, 64, 4, 2, 1), nn.ReLU(),\n","      nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),\n","      nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),\n","      nn.Conv2d(256, 512, 4, 2, 1), nn.BatchNorm2d(512), nn.ReLU()\n","    )\n","    self.fc_mu = nn.Linear(512*4*4, latent_dim)\n","    self.fc_logvar = nn.Linear(512*4*4, latent_dim)\n","\n","  def forward(self, x):\n","    x = self.conv(x)\n","    x = x.view(x.size(0), -1)\n","    return self.fc_mu(x), self.fc_logvar(x)\n","\n","class Decoder(nn.Module):\n","  def __init__(self, latent_dim, out_channels):\n","    super(Decoder, self).__init__()\n","    self.fc = nn.Linear(latent_dim, 512*4*4)\n","    self.deconv = nn.Sequential(\n","      nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(),\n","      nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),\n","      nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),\n","      nn.ConvTranspose2d(64, out_channels, 4, 2, 1), nn.Tanh()\n","    )\n","\n","  def forward(self, z):\n","    x = self.fc(z)\n","    x = x.view(x.size(0), 512, 4, 4)\n","    return self.deconv(x)\n","\n","class VAE(nn.Module):\n","  def __init__(self, latent_dim, in_channels, out_channels):\n","    super(VAE, self).__init__()\n","    self.encoder = Encoder(latent_dim, in_channels)\n","    self.decoder = Decoder(latent_dim, out_channels)\n","\n","  def reparameterize(self, mu, logvar):\n","    std = torch.exp(0.5 * logvar)\n","    eps = torch.randn_like(std)\n","    return mu + eps * std\n","\n","  def forward(self, x):\n","    mu, logvar = self.encoder(x)\n","    z = self.reparameterize(mu, logvar)\n","    return self.decoder(z), mu, logvar\n","\n","model = VAE(latent_dim, input_channels, input_channels).to(device) # Pass input/output channels\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_epochs):\n","  model.train()\n","  total_loss = 0\n","  for images, _ in dataloader:\n","    images = images.to(device)\n","    recon, mu, logvar = model(images)\n","    loss = vae_loss(recon, images, mu, logvar)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    total_loss += loss.item()\n","  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss:{total_loss/len(dataloader.dataset):.4f}\")\n","\n","  model.eval()\n","  with torch.no_grad():\n","    z = torch.randn(64, latent_dim).to(device)\n","    sample_images = model.decoder(z).cpu() * 0.5 + 0.5 # De-normalize\n","    grid = torchvision.utils.make_grid(sample_images, nrow=8, normalize=True)\n","    if input_channels == 1:\n","        plt.imshow(grid.permute(1, 2, 0), cmap='gray') # Use grayscale colormap for MNIST\n","        plt.title(f\"Generated Digits - Epoch {epoch+1}\")\n","    else:\n","        plt.imshow(grid.permute(1, 2, 0)) # Use default colormap for CelebA\n","        plt.title(f\"Generated Faces - Epoch {epoch+1}\")\n","    plt.axis('off')\n","    plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"-RRegE2RUQY0","executionInfo":{"status":"error","timestamp":1761058836327,"user_tz":-330,"elapsed":172,"user":{"displayName":"GANESH S 231501046","userId":"12955083252446110846"}},"outputId":"f3795a49-903e-48ec-af4c-5db877245ff3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"output with shape [1, 64, 64] doesn't match the broadcast shape [3, 64, 64]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4164971903.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Assuming your manual dataloader also returns image and a placeholder for label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 64, 64] doesn't match the broadcast shape [3, 64, 64]"]}]}]}